{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we shall train a sentiment analysis model capable of classifying the attitudes expressed\n",
    "in a film review. The dataset is a sample of the **IMDb** dataset that contains `50,000`\n",
    "reviews (split equally into `25,000` train and `25,000` test sets) of movies accompanied by a\n",
    "label expressing the sentiment of the review (`0 = negative`, `1 = positive`). **IMDb**\n",
    "(https://www.imdb.com/) is a large online database containing information\n",
    "about films, TV series, and video games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **IMDb** textual\n",
    "data offered by Keras is cleansed of punctuation, normalized into lowercase, and\n",
    "transformed into numeric values. Each word is coded into a number representing\n",
    "its ranking in frequency.\n",
    "For the *vocabulary size*, we are ensuring that we want to consider a maximum `10,000-word` vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 10000\n",
    "((x_train, y_train), (x_test, y_test)) = keras.datasets.imdb.load_data(num_words = top_words, seed = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size Training Examples: 25000\n",
      "Size of Training labels: 25000\n",
      "\n",
      "Size Test Examples: 25000\n",
      "Size of Test labels: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size Training Examples: %i\" % len(x_train))\n",
    "print(\"Size of Training labels: %i\\n\" % len(y_train))\n",
    "print(\"Size Test Examples: %i\" % len(x_test))\n",
    "print(\"Size of Test labels: %i\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (array([0, 1], dtype=int64), array([12500, 12500], dtype=int64)) \n",
      "\n",
      "Test set: (array([0, 1], dtype=int64), array([12500, 12500], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print('Train set:', np.unique(y_train, return_counts = True), '\\n')\n",
    "print('Test set:', np.unique(y_test, return_counts = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstration dataset is *balanced* with equal number\n",
    "of positive and negative sentiment examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create some Python\n",
    "dictionaries that can convert between the code used in the dataset and the real words or review texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {w: i + 3 for w, i in keras.datasets.imdb.get_word_index().items()}\n",
    "id_to_word = {0: '<PAD>', 1: '<START>', 2: '<UNK>'}\n",
    "id_to_word.update({i + 3: w for w, i in keras.datasets.imdb.get_word_index().items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_text(sequence):\n",
    "    return ' '.join([id_to_word[s] for s in sequence if s >= 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's convert some samples sequences to texts and get their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this movie was like a bad train wreck as horrible as it was you still had to continue to watch my boyfriend and i rented it and wasted two hours of our day now don't get me wrong the acting is good just the movie as a whole just both of us there wasn't anything positive or good about this scenario after this movie i had to go rent something else that was a little lighter jennifer is as usual a very dramatic actress her character seems manic and not all there hannah though over played she does a wonderful job playing out the situation she is in more than once i found myself yelling at the tv telling her to fight back or to get violent all in all very violent movie not for the faint of heart\n"
     ]
    }
   ],
   "source": [
    "print(convert_to_text(x_train[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score: 0\n"
     ]
    }
   ],
   "source": [
    "print('Sentiment score:', y_train[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as good as list was i found this movie much more powerful as it is a documentary and based on real life it details the story of the frank family and anne in particular although it is a bit slow moving at first their family life before the war it becomes very powerful br br due to some of the footage and photos of the camps i would not recommend it for children but for adults it illustrates the horror of the holocaust through one young girl highly recommended\n"
     ]
    }
   ],
   "source": [
    "print(convert_to_text(x_test[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score: 1\n"
     ]
    }
   ],
   "source": [
    "print('Sentiment score:', y_test[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious from the wordings of the texts that the first is a **negative** review (sentiment `score = 0`) with words like `bad`, `wreck`, `horrible` used to describe it. The second is a **positive** review (sentiment `score = 1`)with words like `good`, `powerful`, `highly recommended` used to describe the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the first few words in a review can reveal the sentiments. Sometimes the sentiment is hidden in the middle or right at the end of the review. In bulding the model we shall limit the number of words to analyse to get the sense. In this example we shall consider the first `200` words and those reviews with less than 200 words shall be **input padded** with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding: \n",
      "\n",
      "[1, 13, 119, 78, 3310, 102, 13, 66, 81, 13, 462, 7273, 33, 98, 5, 4, 6196, 1308, 16, 260, 6, 9337, 7, 98, 9992, 11, 4, 2, 7, 68, 162, 204, 431, 8870, 3310, 9159, 448, 23, 4, 5596, 12, 610, 40, 12, 16, 170, 8, 30, 545, 1139, 2027, 6, 1034, 7, 2, 1664, 66, 12, 16, 2, 34, 6, 800, 7, 3310, 1274, 342, 2, 63, 9, 3310, 20, 5868, 33, 94, 118, 13, 16, 11, 4, 1310, 13, 16, 1623, 8, 140, 721, 12, 23, 8870, 1168, 1656, 132, 449, 558, 16, 15, 20, 355, 10, 10, 355, 355, 355, 10, 10, 1195, 2509, 4621, 56, 10, 10, 14, 9, 2, 2, 33, 94, 55, 249, 61, 369, 54, 6, 8527, 46, 250, 9, 839, 46, 7, 9429, 748, 5, 2, 8, 6, 2702, 1930, 41, 419, 125, 88, 4, 3310, 406, 6762, 2, 4, 427, 2140, 1656, 4042, 2, 11, 41, 2, 494, 46, 1954, 4712, 198, 51, 13, 683, 1193, 10, 10, 198, 66, 89, 4, 114, 495, 7303, 197, 4, 1168, 1656, 61, 492, 1131, 7, 5388, 21, 13, 839, 90, 145, 8, 113, 34, 8253, 27, 2, 19, 15, 7, 6, 8870, 3310, 88, 8222, 92, 2, 8, 5388, 5, 1037, 2, 2, 2864, 2, 449, 168, 6, 404, 2, 112, 207, 1075, 4, 375, 5986, 7, 4, 406, 1522, 13, 124, 903, 97, 90, 2, 21, 2, 48, 32, 148, 3310, 2, 2, 93, 61, 492, 2, 305, 7, 2, 4, 893, 8016, 13, 401, 5679, 83, 27, 117, 2687, 5419, 29, 941, 1889, 90, 21, 808, 14, 46, 793, 4, 1526, 84, 37, 28, 34, 96, 7, 49, 2, 114, 1009, 1054, 56, 23, 61, 2301, 1111, 9, 4, 255, 8, 937, 61, 492, 16, 3953, 159, 29, 1131, 13, 2134, 3872, 81, 41, 32, 14, 832, 56, 8, 35, 576, 1301, 5, 5348, 3134, 255, 335, 170, 8, 2, 72, 1168, 1656, 57, 29, 9, 2, 2, 3310, 415, 11, 5215, 89, 1047, 10, 10, 81, 24, 106, 14, 20, 126]\n"
     ]
    }
   ],
   "source": [
    "print('Before padding: \\n')\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pad = 200\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen = max_pad)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen = max_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding: \n",
      "\n",
      "[  88    4 3310  406 6762    2    4  427 2140 1656 4042    2   11   41\n",
      "    2  494   46 1954 4712  198   51   13  683 1193   10   10  198   66\n",
      "   89    4  114  495 7303  197    4 1168 1656   61  492 1131    7 5388\n",
      "   21   13  839   90  145    8  113   34 8253   27    2   19   15    7\n",
      "    6 8870 3310   88 8222   92    2    8 5388    5 1037    2    2 2864\n",
      "    2  449  168    6  404    2  112  207 1075    4  375 5986    7    4\n",
      "  406 1522   13  124  903   97   90    2   21    2   48   32  148 3310\n",
      "    2    2   93   61  492    2  305    7    2    4  893 8016   13  401\n",
      " 5679   83   27  117 2687 5419   29  941 1889   90   21  808   14   46\n",
      "  793    4 1526   84   37   28   34   96    7   49    2  114 1009 1054\n",
      "   56   23   61 2301 1111    9    4  255    8  937   61  492   16 3953\n",
      "  159   29 1131   13 2134 3872   81   41   32   14  832   56    8   35\n",
      "  576 1301    5 5348 3134  255  335  170    8    2   72 1168 1656   57\n",
      "   29    9    2    2 3310  415   11 5215   89 1047   10   10   81   24\n",
      "  106   14   20  126]\n"
     ]
    }
   ],
   "source": [
    "print('After padding: \\n')\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example the **embedding** will apply a pretrained word embedding (such as **Word2vec** or **GloVe**) to the sequence input.\n",
    "The model uses **Bidirectional** wrapping — an LSTM layer of 64 cells.\n",
    "**Bidirectional**\n",
    "transforms a normal **LSTM** layer by doubling it: On the first side,\n",
    "it applies the normal sequence of inputs you provide; on the second, it passes the\n",
    "reverse of the sequence. We use this approach because sometimes we use words\n",
    "in a different order, and building a bidirectional layer will catch any word pattern,\n",
    "no matter the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 32\n",
    "\n",
    "model = keras.models.Sequential() \n",
    "model.add(keras.layers.Embedding(top_words, embedding_vector_length, input_length = max_pad))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences = True)))\n",
    "model.add(keras.layers.GlobalMaxPool1D())\n",
    "model.add(keras.layers.Dense(16, activation = \"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 32)           320000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 128)          49664     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 371,745\n",
      "Trainable params: 371,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = batch_size, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 51s 2ms/sample - loss: 0.0246 - accuracy: 0.9963\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.63%\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: {}%'.format(round(float(train_acc) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 50s 2ms/sample - loss: 0.5602 - accuracy: 0.8504\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 85.04%\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: {}%'.format(round(float(test_acc) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just trained a sentiment analyzer that can guess the sentiment\n",
    "expressed in a movie review correctly about `85%` of the time. With `15%` chances of wrongfully guessing the sentiment in a review does not make this model a very impressive one. However, with\n",
    "more training data and a corresponding more complex neural architectures, we can get results that are even more impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can extract the *embeddings* learned from the model, using the *layers* function. Each embedding is a vector of size `32`, and we have `10,000` embeddings, since our total vocabulary size was set to `10,000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_2/embeddings:0' shape=(10000, 32) dtype=float32, numpy=\n",
       " array([[ 0.01176346, -0.02211049,  0.01999186, ..., -0.00767972,\n",
       "          0.00915258,  0.02567264],\n",
       "        [ 0.03514829, -0.00280277, -0.02752534, ...,  0.01723774,\n",
       "          0.03138287, -0.02131016],\n",
       "        [ 0.00956264, -0.00240009, -0.03587588, ..., -0.00217557,\n",
       "          0.07052197,  0.03453688],\n",
       "        ...,\n",
       "        [-0.07937256,  0.10807124, -0.01995642, ...,  0.0930188 ,\n",
       "          0.09878337, -0.06895982],\n",
       "        [-0.0682204 ,  0.00144474,  0.09145122, ...,  0.07640875,\n",
       "          0.03292344,  0.01669248],\n",
       "        [ 0.06022037, -0.05517315, -0.03429912, ..., -0.09712823,\n",
       "         -0.0608963 ,  0.00122278]], dtype=float32)>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.layers[0]\n",
    "embeddings.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shape: (10000, 32)\n"
     ]
    }
   ],
   "source": [
    "weights = embeddings.get_weights()[0]\n",
    "print('Weights shape:', weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.5148285e-02, -2.8027729e-03, -2.7525343e-02, -6.6123626e-05,\n",
       "       -1.8618425e-02, -1.9484749e-02, -5.7897922e-02, -6.1920997e-02,\n",
       "       -5.7299681e-02,  2.9555941e-02, -1.9974085e-03, -5.9607994e-02,\n",
       "        2.9678049e-02,  3.8764924e-02, -7.4368590e-03,  3.2156970e-02,\n",
       "       -1.8604781e-02,  4.0184289e-02,  1.3975586e-02,  6.7665675e-03,\n",
       "       -6.0407475e-02, -4.2519130e-02, -2.9203249e-02, -1.3698910e-04,\n",
       "        1.5641183e-02, -2.7326887e-02,  8.7939892e-03,  1.0132346e-03,\n",
       "       -3.4886673e-02,  1.7237742e-02,  3.1382874e-02, -2.1310160e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the embeddings in the 3D space, we must reverse the key value for embeddings and respective words, so as to represent every word via its embedding. To do this, we create a helper function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_based_embedding = dict([(value, key) for (key, value) in word_to_id.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return ' '.join([index_based_embedding.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_based_embedding[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final part of this exercise, we extract the embeddings value and put it into a `.tsv` file, along with another `.tsv` file that captures the words of the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = io.open('./data/embedding_vectors_new.tsv', 'w', encoding = 'utf-8')\n",
    "meta = io.open('./data/metadata_new.tsv', 'w', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "for i in range(1, vocab_size):\n",
    "    if i in index_based_embedding.keys():\n",
    "        word = index_based_embedding[i]\n",
    "        embedding_vec_values = weights[i]\n",
    "        meta.write(word + \"\\n\")\n",
    "        vec.write('\\t'.join([str(x) for x in embedding_vec_values]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta.close()\n",
    "vec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the embeddings in 3D space using TensorFlow Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the embeddings go to https://projector.tensorflow.org/ and load `embedding_vectors_new.tsv` and `metadata_new.tsv`. Try a positive word such as *recommended* and a negative world like *boring* and see how related words are distributed around them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './model/sentiment_model.h5'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prediction = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Review Text preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \", str(text))\n",
    "    return re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = [\"Loved the film. I wasn’t sure at the start but it was lovely \"+ \\\n",
    "               \"to see all the characters from the small screen arrive in the \"+ \\\n",
    "               \"cinema as old friends, and I laughed and cried. This is a great \"+ \\\n",
    "               \"film and I really hope they make a sequel. \"+ \\\n",
    "               \"To the person who gave this film one star you should have reviewed \"+ \\\n",
    "               \"the film, not the taxi driver and as you didn’t see the first 30 minutes \"+ \\\n",
    "               \"you aren’t in a position to comment on the entire film anyway.\", \n",
    "               \n",
    "               \"I have no doubt that Uptown fans will support this film. We have every \"+ \\\n",
    "               \"episode on DVD, so it is with something of a heavy heart to give this film \"+ \\\n",
    "               \"such a low rating. As a stand-alone film (or if you have never seen the TV series), \"+ \\\n",
    "               \"all you get are lavish scenery and costumes. However, the characters appear shallow \"+ \\\n",
    "               \"and the plot flimsy. As a Uptown fan, yes – the pleasant and familiar characters are \"+ \\\n",
    "               \"there for you to enjoy in their familiar costumes. However, that is not enough. \"+ \\\n",
    "               \"Soon into the film, we found that the depth of our characters was not there. \"+ \\\n",
    "               \"I can only allude to metaphors. It was like watching a Formula One race run at \"+ \\\n",
    "               \"20 mph – where was the excitement? It was like watching a Weakenhand ruby game of \"+ \\\n",
    "               \"touch rugby – all spectacle but no impact. It was like being forced to lie in a bubble \"+ \\\n",
    "               \"bath of lukewarm water for too long. Even a couple of people around gave up and stated \"+ \\\n",
    "              \"playing with their iPhone, with mutterings as we left at it was far too long. \"+ \\\n",
    "               \"Maybe it was our local cinema’s projection but even the film quality was nothing \"+ \\\n",
    "               \"like my Blu-ray at home, let along 4K. So, all in all, this is best seen as a light touch \"+ \\\n",
    "               \"homage to the TV series. Bearing in mind the trouble taken to assemble the actors in one \"+ \\\n",
    "               \"place at one time to make this movie, this was a wasted opportunity to create a real Uptown epic. \"+ \\\n",
    "               \"I hope they do not make a sequel.\"\n",
    "               ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_clean = [] \n",
    "for i in range(len(review_text)):\n",
    "    review_text_clean.append(clean_reviews(review_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 183 unique words in the review text.\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(word.lower() for phrase in review_text_clean for word in phrase.split(\" \"))\n",
    "print(f\"There are {len(unique_words)} unique words in the review text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(unique_words) + 1\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words = vocabulary_size, oov_token = 'xxxxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_dict: 183\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(review_text_clean)\n",
    "review_dict = tokenizer.word_index\n",
    "print('Length of X_dict:', len(review_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('xxxxxxx', 1), ('the', 2), ('a', 3), ('to', 4), ('film', 5), ('was', 6), ('and', 7), ('it', 8), ('in', 9), ('this', 10), ('i', 11), ('as', 12), ('you', 13), ('at', 14), ('all', 15), ('of', 16), ('characters', 17), ('is', 18), ('one', 19), ('have', 20), ('not', 21), ('like', 22), ('t', 23), ('but', 24), ('make', 25), ('that', 26), ('uptown', 27), ('we', 28), ('with', 29), ('see', 30), ('cinema', 31), ('hope', 32), ('they', 33), ('sequel', 34), ('gave', 35), ('on', 36), ('no', 37), ('so', 38), ('seen', 39), ('tv', 40), ('series', 41), ('are', 42), ('costumes', 43), ('however', 44), ('familiar', 45), ('there', 46), ('for', 47), ('their', 48), ('our', 49), ('watching', 50), ('touch', 51), ('too', 52), ('long', 53), ('even', 54), ('loved', 55), ('wasn', 56), ('sure', 57), ('start', 58), ('lovely', 59), ('from', 60), ('small', 61), ('screen', 62), ('arrive', 63), ('old', 64), ('friends', 65), ('laughed', 66), ('cried', 67), ('great', 68), ('really', 69), ('person', 70), ('who', 71), ('star', 72), ('should', 73), ('reviewed', 74), ('taxi', 75), ('driver', 76), ('didn', 77), ('first', 78), ('minutes', 79), ('aren', 80), ('position', 81), ('comment', 82), ('entire', 83), ('anyway', 84), ('doubt', 85), ('fans', 86), ('will', 87), ('support', 88), ('every', 89), ('episode', 90), ('dvd', 91), ('something', 92), ('heavy', 93), ('heart', 94), ('give', 95), ('such', 96), ('low', 97), ('rating', 98), ('stand', 99), ('alone', 100), ('or', 101), ('if', 102), ('never', 103), ('get', 104), ('lavish', 105), ('scenery', 106), ('appear', 107), ('shallow', 108), ('plot', 109), ('flimsy', 110), ('fan', 111), ('yes', 112), ('pleasant', 113), ('enjoy', 114), ('enough', 115), ('soon', 116), ('into', 117), ('found', 118), ('depth', 119), ('can', 120), ('only', 121), ('allude', 122), ('metaphors', 123), ('formula', 124), ('race', 125), ('run', 126), ('mph', 127), ('where', 128), ('excitement', 129), ('weakenhand', 130), ('ruby', 131), ('game', 132), ('rugby', 133), ('spectacle', 134), ('impact', 135), ('being', 136), ('forced', 137), ('lie', 138), ('bubble', 139), ('bath', 140), ('lukewarm', 141), ('water', 142), ('couple', 143), ('people', 144), ('around', 145), ('up', 146), ('stated', 147), ('playing', 148), ('iphone', 149), ('mutterings', 150), ('left', 151), ('far', 152), ('maybe', 153), ('local', 154), ('s', 155), ('projection', 156), ('quality', 157), ('nothing', 158), ('my', 159), ('blu', 160), ('ray', 161), ('home', 162), ('let', 163), ('along', 164), ('k', 165), ('best', 166), ('light', 167), ('homage', 168), ('bearing', 169), ('mind', 170), ('trouble', 171), ('taken', 172), ('assemble', 173), ('actors', 174), ('place', 175), ('time', 176), ('movie', 177), ('wasted', 178), ('opportunity', 179), ('create', 180), ('real', 181), ('epic', 182), ('do', 183)])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_seq = tokenizer.texts_to_sequences(review_text_clean)\n",
    "#review_seq[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding: \n",
      "\n",
      "[55, 2, 5, 11, 56, 23, 57, 14, 2, 58, 24, 8, 6, 59, 4, 30, 15, 2, 17, 60, 2, 61, 62, 63, 9, 2, 31, 12, 64, 65, 7, 11, 66, 7, 67, 10, 18, 3, 68, 5, 7, 11, 69, 32, 33, 25, 3, 34, 4, 2, 70, 71, 35, 10, 5, 19, 72, 13, 73, 20, 74, 2, 5, 21, 2, 75, 76, 7, 12, 13, 77, 23, 30, 2, 78, 79, 13, 80, 23, 9, 3, 81, 4, 82, 36, 2, 83, 5, 84]\n"
     ]
    }
   ],
   "source": [
    "print('Before padding: \\n')\n",
    "print(review_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_padded_seq = keras.preprocessing.sequence.pad_sequences(review_seq, maxlen = max_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding: \n",
      "\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 55  2  5 11 56 23 57 14  2\n",
      " 58 24  8  6 59  4 30 15  2 17 60  2 61 62 63  9  2 31 12 64 65  7 11 66\n",
      "  7 67 10 18  3 68  5  7 11 69 32 33 25  3 34  4  2 70 71 35 10  5 19 72\n",
      " 13 73 20 74  2  5 21  2 75 76  7 12 13 77 23 30  2 78 79 13 80 23  9  3\n",
      " 81  4 82 36  2 83  5 84]\n"
     ]
    }
   ],
   "source": [
    "print('After padding: \\n')\n",
    "print(review_padded_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_padded_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20508002]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_pred0 = sentiment_prediction.predict(review_padded_seq[0:1])\n",
    "review_pred0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01894105]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_pred1 = sentiment_prediction.predict(review_padded_seq[1:2])\n",
    "review_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2050801 ],\n",
       "       [0.01894104]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_pred = sentiment_prediction.predict(review_padded_seq)\n",
    "review_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2050801 ],\n",
       "       [0.01894104]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_pred_prob = sentiment_prediction.predict_proba(review_padded_seq)\n",
    "review_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
